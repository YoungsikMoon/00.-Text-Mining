{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkOSWH0yZXkK6zSy9qjEes",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoungsikMoon/PublicRnD/blob/main/%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D_%EB%AC%B8%EC%98%81%EC%8B%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#텍스트 마이닝에 필요한 최소 지식 정리"
      ],
      "metadata": {
        "id": "T6pGRyzrnA22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래 내용은 성수 알파코 캠퍼스 9기 텍스트마이닝 교육자료 약 95%(저자:류영표 강사)와 구글 검색5% 정도를 가지고 재해석하여 작성하였음을 참고바람.\n",
        "\n",
        "주의!! 재해석 과정에서 개인의 주관이 반영되어 올바른 내용이 아닐 수 있음.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qlDId3Oo2E3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 마이닝이란? 텍스트에서 의미 있는 정보를 얻는 작업\n",
        "\n",
        "텍스트 마이닝 과정 요약 :\n",
        "텍스트 데이터 수집 -> 텍스트 전처리 -> 특징 값 추출 -> 결과 데이터 분석\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BnM4kFB7pQRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "보통 텍스트 마이닝 하면 '자연어처리'라 생각할 수 있지만, 다른 분야이다. 이 글에서는 텍스트마이닝 위주로 작성할 것 이다.\n",
        "\n",
        "자연어 : 사람이 일상에서 자연스럽게 사용하는 언어\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "bC5tPhkX0VNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "자연어처리 (NLP, Natural Language Processing) : 컴퓨터가 인간의 언어인 자연어를 인공지능이 인식하고 표현할 수 있도록 하는 것에 중점을 둠.\n",
        "\n",
        "텍스트마이닝 : 텍스트데이터에서 추출된 단어나 문장속에서 유의미한 인사이트를 찾아내는 작업.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vib7jiUDnvsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 데이터 수집 : 인증 및 허가 된 사이트에서 텍스트로 된 데이터를 받기 또는 웹 스크랩핑(윤리적 문제에 주의)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HmLFenXYort3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 전처리란?\n",
        "수집한 데이터를 분석하기에 앞서 분석에 용이하도록 데이터를 정제/가공하고 텍스트의 의미를 충분히 간직한 채로 컴퓨터가 이해할 수 있는 형태로 전환하는 작업.\n",
        "\n",
        "토큰화, 오탈자, 특수문자, 띄어쓰기 교정, 맞춤법 검사 등 데이터 분석 용도에 맞게 텍스트를 사전에 처리하는 작업.\n",
        "\n",
        "텍스트 분석 모델이 효율적으로 작동할 수 있도록 사전에 텍스트를 정제 및 가공.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "go5SYBP4pB8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "코퍼스(말뭉치) : 특정한 목적을 가지고 언어 표본을 추출한 집합\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9fyVeuIs_hxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "토큰화 : 텍스트를 원하는 단위로 분절하는 작업.\n",
        "\n",
        "문장 토큰화 : 문장의 마침표, 개행문자, 느낌표, 물음표 등 문장의 마지막을 뜻하는 기호에 따라 분절하는 것이 일반적.\n",
        "\n",
        "단어 토큰화 : 기본적으로 띄어쓰기를 기준으로 하며, 구두점(온점, 느낌표, 물음표, 쉼표 등)을 토큰화하기도 함.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lFTh20mSqEOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "정제/정규화 : 토큰화 전/후 에 하는 작업\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "bnF03McfpxXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "정제(노이즈/불용어 제거) : 데이터에서 큰 의미가 없는 단어 제거.\n",
        "\n",
        "음, 뭐, 아\n",
        "\n",
        "등장 빈도가 적은 데이터, 의미가 없는 특수문자 등\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mPxszAcIqzMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "정규화(Nomalization) : 표현 방법이 다른 단어들을 통합 시킴\n",
        "\n",
        "uh-huh 와 uhhuh는 형태는 다르지만 같다. 통일 시켜줄 필요가 있다.\n",
        "\n",
        "대부분의 글은 소문자로 작성되어있다. 불필요하게 대문자를 섞기보단 분석하기 용이하게 전부 소문자로 변환하는게 좋다.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "KH_RGJuYqoJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "음소 : 자음 or 모음\n",
        "\n",
        "음운론 상의 최소 단위\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jJtMtuNtqzHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "음절 : 말하는 사람, 듣는 사람이 한 뭉치로 생각하는 발화의 단위.\n",
        "\n",
        "한 글자\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "S1zyICKuqzC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "어절 : 띄어쓰기 단위\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TaVtx65ksepI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "★ 형태소 : 의미가 있는 가장 작은 단위\n",
        "\n",
        "아버지/가/방/에/들어가/신다\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fvLhOj1tqy3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "낱말(단어) : 자립성이 있는 최소 단위\n",
        "\n",
        "아버지가/방에/들어가신다\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HVG-iqQAsRvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "한국어 단어 토큰화 = 형태소 분석\n",
        "\n",
        "자립 형태소 : 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등 조사나 어미 없이 단어 자체를 사용.\n",
        "\n",
        "예) 강 물 매우 주먹 밥\n",
        "\n",
        "의존 형태소 : 접사, 어미, 조사, 어간처럼 다른 형태소와 결합해 사용\n",
        "\n",
        "예) -을, -가, -는, -은\n",
        "\n",
        "뒤에 자세히 나오지만 한국어는 분석이 매우 어려운 편!\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-6bafLQGua5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "한국어 전처리&분석 관련 패키지가 아래처럼 다양하게 많으니까 너무 걱정은 말자.\n",
        "\n",
        "아래 것들이 있다고 정도만 참고.\n",
        "\n",
        ". KoNLPy : 다양한 형태소 분석기를 제공하는 파이썬 패키지.Okt(Open Korea Text), Hannanum, Kkma, Komoran, Mecab 등 포함 됨.\n",
        "\n",
        ". Okt : 트위터분석기라는 이름으로 진행되었던 프로젝트가 fork되어 진행되는 것으로 기본 베이스는 트위터 분석기\n",
        "\n",
        ". Hannanum : KAIST에서 개발한 형태소 분석기\n",
        "\n",
        ". Mecab : 일본어 형태소 분석기를 한국에 맞게 수정한 버전.\n",
        "\n",
        ". Komoran : Java로 작성된 형태소 분석기,\n",
        "\n",
        ". Soynlp : 비지도 학습을 기반으로 하는 한국어 전용 토큰화 도구. 데이터 내에서 자주 등장하는 단어를 기반으로 하는 토큰화를 수행.\n",
        "\n",
        ". Kkma : 서울대학교에서 만든 분석기, 성능은 좋으나 속도면에서 단점이 있다.\n",
        "\n",
        ". Khaiii : 카카오에서 개발한 형태소 분석기. 딥러닝 기반으로 정확도가 높음.\n",
        "\n",
        ". KSS(Korean Sentence Splitter) : 한국어 문장 분리를 위한 도구. 문장의 경계를 잘 인식하여 텍스트를 문장 단위로 나누는데 유용.\n",
        "\n",
        ". ETRI : 한국전자통신연구원에서 개발한 한국어 처리 API. 형태소 분석, 개체명 인식, 의존 구문분석 등 다양한 기능을 제공. 인터넷 연결이 필요.\n",
        "\n",
        ". Pororo : 카카오브레인에서 만든 한글 자연어처리 라이브러리.\n",
        "\n",
        ". KoBERT : 한국어 정보 처리를 위해 특별히 훈련된 BERT 모델. 토큰화 기능 외에도 한국어 텍스트의 이해와 생성 작업에 탁원한 성능을 보임.\n",
        "\n",
        ". Kiwi : 다양한 자연어 처리 기능을 제공. 형태소 분석, 품사 태깅 등 지원.\n",
        "\n",
        ". KoGPT-2 : 한국어 생성 모델, 자연어 이해 및 생성 작업에 사용됨. 대화 생성, 문장 완성 등의 작업에 적합.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xCdjvAWpY8xQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "정규화 : 형태는 다르지만 같은 의미를 가진 단어를 하나로 통일시켜주는 작업\n",
        "\n",
        "중복된 단어를 하나로 통합하여 데이터를 줄임.\n",
        "\n",
        "Apple, apple -> apple (소문자로 통일 시켜줌)\n",
        "\n",
        "going, goes -> go\n",
        "\n",
        "갔다, 갔었다 -> 가다\n",
        "\n",
        "어간 추출, 표제어 추출 로 가능.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Owe1Zd4xuaxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "어간(Stem) :\n",
        "\n",
        "가다, 간다, 갔다, 갔었다, 갈 것이다 -> 가\n",
        "go, goes, going -> go\n",
        "\n",
        "NLTP 패키지의 어간 추출기 : PoterStemmer, LancasterStemmer\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CMs6S2pNuav0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "표제어(Lemmatization) : 단어의 기본형\n",
        "\n",
        "goes -> go\n",
        "\n",
        "has -> have\n",
        "\n",
        "가다,간다,갔다,갔었다,갈 것이다 ->  가다\n",
        "\n",
        "NLTP 패키지의 표제어 추출기 : WordNetLemmatizer\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "y0Ce3f7Uuat4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "품사(pos): 기능, 형태, 의미에 따라 단어를 구분\n",
        "\n",
        "한글 품사 : 명사, 대명사, 수사, 동사, 형용사, 관형사, 부사, 감탄사, 조사\n",
        "\n",
        "NLTP 품사 : NOUN, PRON, NUM, VERB, ADJ, DET, ADV, PRT, CONJ, ADP\n",
        "\n",
        "언어학자 또는 연구자에 따라 품사 종류가 다르다.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "D34DrgNyuar2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "품사 태깅 : 단어 또는 형태소에 품사를 붙이는 작업\n",
        "\n",
        "NLTK 에서는 pos_tag() 함수를 지원함.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wLJ_teSQuap6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "벡터화(Vectorize) : 텍스트를 컴퓨터가 이해할 수 있도록 벡터로 만들어주는 것, 텍스트 입력을 수치 표현으로 변환하는 것.\n",
        "\n",
        "벡터화에는 다양한 방식이 존재하는데 그중 기본적으로 아래 것들이 있다.\n",
        "\n",
        "BoW(Bag-of-Words), DTM(Document-Term Matrix) ,TF-IDF(Term Frequency-Inverse Document Frequency), Word Embedding\n",
        "\n",
        "벡터화를 알기 위해선 원-핫 인코딩, 희소표현 이란 개념을 알고가면 좋다.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "UknKYGgSuaoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "원-핫 인코딩 : 사과 배 수박 가 있다면.\n",
        "\n",
        "사과 [ 1, 0, 0]\n",
        "배 [0, 1, 0]\n",
        "수박 [0, 0, 1]\n",
        "\n",
        "---\n",
        "\n",
        "이렇게 되면 되면 적은 단어나 문장을 원-핫 인코딩 하기엔 좋지만.. 문장이 길어지거나 단어가 많아진걸 원-핫 인코딩 하면 0이 무수히 많아진다.\n",
        "\n",
        "단어 10,000개 가 있다고 한다면..\n",
        "\n",
        "사과 [0,0,0,0,0,0, ....0,1]\n",
        "이런걸 '희소'라고 한다.\n",
        "\n",
        "이렇게 표현하면 데이터관리가 힘들어진다. 이 걸 희소표현(Sparse Matrix) 문제라고 한다.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j3bs4c8DGUg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW : 단어의 등장 순서는 고려하지 않고, 단어들의 출현 빈도에만 집중하는 텍스트 데이터의 수치화 표현 방법.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vWjzQcM7uakO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DTM : 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0iuZL5viBrgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF : 전체 코퍼스와 비교하여 문서에 있는 각 단어의 중요치 계산.\n",
        "\n",
        "문서 내 단어의 빈도수가 높더라도 다른 문서에서도 많이 등장하는 단어라면 TF-IDF 값은 낮게 나옴.\n",
        "\n",
        "문서 내 단어의 빈도수가 높은 단어들 가운데, 다른 문서에서는 잘 등장하지 않는 단어의 TF-IDF 값이 높게 나옴.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wVIgYcK4uamE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Embedding : 희소표현 문제를 개선하여 나온 것.\n",
        "\n",
        "대표적으로 word2vec, GloVe 등이 있다.\n",
        "\n",
        "희소표현 대신 밀집표현(Dense Matrix)을 하며. 값은 실수가 된다. 단어의 관계 학습이 필요한 방식이다.\n",
        "\n",
        "자세한건 뒤에.. 실습 쪽에서 보여줌.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "f01ihOt5uaXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-gram : n개의 연속된 단어를 새로운 하나의 단어로 정의\n",
        "\n",
        "2개의 단어를 하나로 묶으면 2-gram (=bi-gram)\n",
        "\n",
        "3개의 단어를 하나로 묶으면 3-gram (=tri-gram)\n",
        "\n",
        "그 이상의 단어를 하나로 묶으면 :벡터 크기가 커지고 희소문제가 생길 수 있어서 2~3 정도만 사용한다.\n",
        "\n",
        "N-gram 목적 : 단어들의 순서를 고려할 수 있게 하기 위한 방법이다. 완벽하진 않지만, 부분적으로 단어의 순서 정보를 반영하여 분석할 수 있다.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mN6l0kEaJkCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트 전처리 실습"
      ],
      "metadata": {
        "id": "lVYdM8ZvLCg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트마이닝 과정은\n",
        "\n",
        "텍스트데이터수집 -> 텍스트 전처리 -> 분석 -> 결과\n",
        "\n",
        "텍스트데이터가 있다고 가정하고 그 다음 단계인 텍스트 전처리(가공)부터 알아보겠다.\n",
        "\n",
        "참고로 텍스트 전처리는 다양한 방법이 있으니 각각 천천히 알아보자."
      ],
      "metadata": {
        "id": "PtEev2UrNy8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "토큰화 실습\n",
        "\n",
        "토큰화에 사용해볼 패키지 : spaCy\n",
        "\n",
        "spaCy : 고급 자연어를 위한 다양한 패키지 中 하나!! (굉장히 많은 패키지가 있음)\n",
        "\n",
        "spaCy의 자세한 정보 : https://spacy.io\n",
        "\n",
        "spaCy로 실습 해보는 이유 : 서론에서 나온 토큰화, 품사태깅, 표제어처리 기능이 있어서 한 번 사용해보려고. (작성한 기능 외에도 다양한 기능을 제공함.)"
      ],
      "metadata": {
        "id": "bo0qGl3nLElu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 전처리 실습에 필요한 패키지(spaCy) 설치 방법\n",
        "# 코랩은 리눅스 환경이다. 때문에 커맨드 맨 앞에 '!'를 붙이면 리눅스 커맨드라고 명시하는 것 임.\n",
        "!pip install spacy\n",
        "# 즉, pip install 명령라인으로 spacy를 설치하라는 것."
      ],
      "metadata": {
        "id": "BcAtJGojpA1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 패키지 설치 완료 확인\n",
        "!pip show spacy"
      ],
      "metadata": {
        "id": "VRwCbwlc4TnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spacy 패키지에서 제공하는 토큰화에 필요한 도구 다운로드\n",
        "# 아래 명령라인은 도구를 다운로드하는 코드일 뿐! 그냥 따라 치면 다운 받아짐.\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "zkpAzsyG48H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화에 필요한 도구 불러온다.\n",
        "import spacy\n",
        "spacy_en = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "bluo2weN5lFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화 해 볼 문장을 적어보자\n",
        "text = \"David's book wasn't famous, but his family loved his book\""
      ],
      "metadata": {
        "id": "3Q67ywqD6AUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화 도구로 토큰화 해보고 토큰된 단어를 출력해보자\n",
        "for token in spacy_en.tokenizer(text): # .tokenizer 단어 토큰화\n",
        "    print(token)\n",
        "# 출력된 print를 보고 문장의 단어가 어떻게 토큰화가 되었는지 본다.\n",
        "# spacy의 en_core_web_sm 토큰화는 출력처럼 나오지만, 또 다른 다양한 언어처리 패키지의 다양한 토큰 도구들은 각각 다른 방식으로 토큰화 한다는 점 기억하자."
      ],
      "metadata": {
        "id": "zkzFm7YQ6g44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "g2ad3OTU7iqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 품사 태깅 및 표제어 추출 실습\n",
        "\n",
        "토큰화에 사용해볼 패키지 : spaCy\n",
        "\n",
        "사용하는 이유 : 앞에 사용한 spaCy에도 품사태깅, 표제어추출 도구가 있어서 써보기."
      ],
      "metadata": {
        "id": "6HHR2xfv7kbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in spacy_en(text):\n",
        "    print(\"토큰 / 품사 / 세분화 된 품사 / 표제어\")\n",
        "    print(f\" {token.text} / {token.pos_} / {token.tag_} / {token.lemma_}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "T5hQFRrf6yM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "r6PqSmDL_m1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "불용어 처리 실습\n",
        "\n",
        "실습에 사용할 패키지 : spaCy\n",
        "\n",
        "사용한 이유 : 패키지 spaCy는 불용어 사전이 미리 만들어져 있으니 한번 연습삼아 써보기"
      ],
      "metadata": {
        "id": "YvFxD_OB_oX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#우선 spacy가 제공하는 불용어 사전을 불러오자\n",
        "spacy_stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# 불러온 불용어 사전에서 어떤것들이 있는지 5개만 확인해보자\n",
        "temp_stop_words = [] #임시 불용어 사전\n",
        "for i, stop_word in enumerate(spacy_stop_words): #enumerate는 인덱스를 부여하는 것임 테스트 편의상 써봄\n",
        "    if i == 5 :break\n",
        "    print(stop_word)\n",
        "    temp_stop_words.append(stop_word) # 지금 확인된 불용어는 regarding nobody below over am 이렇게 5가지를 확인했다.. 분명 더 많겠지만! 이것만 임시 불용어 사전에 담아보자"
      ],
      "metadata": {
        "id": "GKxBsZXT8yJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 임시 불용어는 출력되지 않게 해보자\n",
        "text = \"나는 책을 regarding 하는것이 너무 귀찮아 혹시 nobody 날 도와줄 below 는 없을까? 그럼 이 순간이 over 될 텐뎅\" #영어를 몰라서 그냥 막 써본 텍스트임..\n",
        "filtered_tokens = ' '.join([token.text for token in spacy_en(text) if token.text not in temp_stop_words]) #텍스트를 토큰한 단어들이 불용어 사전에 없으면 토큰을 출력하자.\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "id": "RlFu4j8HBDf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tSNU8c5WF5_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK(Natural Language Toolkit) 토큰화 경험 해보기\n",
        "\n",
        "NLTK : 교육용으로 문서 분석을 비롯해 자연어 처리까지 경험해 볼 수 있는 파이썬 패키지\n",
        "\n",
        "사용해보는 이유 : spacy 말고도 다양한 언어 처리 도구들이 있으니 한번 써보자\n",
        "\n",
        "자세한 정보는 : https://www.lntk.org"
      ],
      "metadata": {
        "id": "c_a7pjd3F7U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK 설치\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "y825jHEICd9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk 설치 확인\n",
        "!pip show nltk"
      ],
      "metadata": {
        "id": "V_5epuQWGhG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk에서 사용해볼 도구들 다운로드\n",
        "import nltk\n",
        "nltk.download('popular') # nltk에 인기있는 도구 모음. (nltk 버전에 따라 인기도구가 다르니 홈페이지 확인 해보자)\n",
        "# 만약 popular 대신 all 를 입력하면 모든 도구를 받아오기 때문에 오래걸린다."
      ],
      "metadata": {
        "id": "EzGBExyoGm-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화에 사용해볼 공통 텍스트\n",
        "# text = \"Can't wait for the summer-sun! :) #excited\"\n",
        "text = \"Looking forward to the summer-sunshine,😎 can't-miss the beach-days & BBQ-nights! #SummerVibes ✨\"\n",
        "# text = \"Can't wait for the summer-break party at John's place! 😊 It's going to be epic - sun, sea, and surf. #SummerVibes 🌞🏄‍♂️\"\n",
        "# text = \"his sentence incorporates a variety of elements that can challenge different tokenizers, including contractions (\\\"Can't\\\"), possessives (\\\"John's\\\"), hyphens (\\\"summer-break\\\"), emoticons (\\\"😊\\\"), and hashtags (\\\"#SummerVibes\\\")\""
      ],
      "metadata": {
        "id": "4jfpcAu4KSaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize #nltk에서 word_tokenize 불러오기\n",
        "print(text)\n",
        "print(word_tokenize(text)) # nltk의 word_tokenize는 쉼표도 하나의 단어로 토큰화 하고있다. I'm에서 'm 도 하나의 단어로 묶어서 처리하고있다."
      ],
      "metadata": {
        "id": "ep3_5OzBJBHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import WordPunctTokenizer #nltk에서 WordPunctTokenize 불러오기\n",
        "print(text)\n",
        "print(WordPunctTokenizer().tokenize(text)) # WordPuctTokenize는 쉼표, 어퍼스트로피 를 하나의 토큰으로 인지해준다. I'm 을 [i, ', m] 이렇게"
      ],
      "metadata": {
        "id": "VBfYrgTxKF3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 어퍼스트로피를 다르게 처리한 텐서플로우의 케라스의 어떤 도구가 있는데 잠깐 소개 해보겠다. 가볍게 이해하자!\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "print(text)\n",
        "print(text_to_word_sequence(text)) #어퍼스트로피로 단어를 연결한걸 하나의 토큰으로 인지한다는 점.\n",
        "# 여기서 말하고자 하는건 다양한 토큰 방식이 있으니 도메인마다 필요에 맞게 여러 도구를 탐색해보라는 의미이다."
      ],
      "metadata": {
        "id": "m7JJA9z8LhP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import TreebankWordTokenizer #표준으로 쓰이고 있는 토큰화 방법 중 하나.\n",
        "print(text)\n",
        "print(TreebankWordTokenizer().tokenize(text))"
      ],
      "metadata": {
        "id": "8RYjhvuWPxtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import TweetTokenizer #이모티콘 인식해주는 토크나이저\n",
        "print(text)\n",
        "print(TweetTokenizer().tokenize(text))"
      ],
      "metadata": {
        "id": "dtjQyFkpUF-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize #nltk 패키지 중 sent_tokenize 불러오기\n",
        "print(text)\n",
        "print(sent_tokenize(text)) # nltk의 sent_tokenize는 문장의 마침표를 기준으로 문장 토큰화 하고있다."
      ],
      "metadata": {
        "id": "NZZ3Fq6jIPkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams # n개의 어절이나 음절을 연쇄적으로 분류\n",
        "text = \"There is no royal road to learning\"\n",
        "bigram = list(ngrams(text.split(),2)) #2개를 묶으면 바이그램\n",
        "print(bigram)"
      ],
      "metadata": {
        "id": "Z72rBIfufZcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams # n개의 어절이나 음절을 연쇄적으로 분류\n",
        "text = \"There is no royal road to learning\"\n",
        "trigram = list(ngrams(text.split(),3)) #3개를 묶으면 트리그램 보통 trigram 까지만 한다.\n",
        "print(trigram)"
      ],
      "metadata": {
        "id": "cd9MVALUf7CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk에서 제공하는 불용어 사전 다운로드\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print(\"nltk 영어 불용어 갯수 : \" , len(nltk.corpus.stopwords.words('english'))) #179개\n",
        "print(nltk.corpus.stopwords.words('english')[:40]) #40개만 보기\n",
        "# 영어의 전치사, 한국어의 조사 등은 분석에 필요하지 않은 경우가 많음\n",
        "# 길이가 짧거나 등장 빈도수가 적은 단어들도 분석에 큰 영향이 없음.\n",
        "# 여러 패키지에서 불용어 코퍼스가 있긴하지만 완벽하진 않음.\n",
        "# 도메인에 맞게 코퍼스를 만들어 사용하는게 가장 좋음. (기존 도구들이 걸러주지 못하는 단어를 추가 하라는 의미)"
      ],
      "metadata": {
        "id": "OS8lHY4hgt6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk는 한국어 불용어 코퍼스를 만들지 않았음.\n",
        "# 한번 간단하게 직접 한국어 불용어 코퍼스를 만들어보자\n",
        "\n",
        "ko_text = \"오늘은 4월 4일 텍스트마이닝 개인 과제를 제출해야한다. 난 지금 밤새 작성중이다. 요즘 벚꽃 폈는데 이쁘더라 첫사랑이 떠오르네\"\n",
        "ko_stop =\"오늘은 개인 난 지금 밤새 요즘 폈는데\"\n",
        "\n",
        "ko_tokens = word_tokenize(ko_text)\n",
        "ko_stop_words = ko_stop.split(' ')\n",
        "\n",
        "result = []\n",
        "for w in ko_tokens:\n",
        "    if w not in ko_stop_words:\n",
        "        result.append(w)\n",
        "\n",
        "print(ko_tokens,'\\t')\n",
        "print(result)"
      ],
      "metadata": {
        "id": "J79olFl8kK88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "사실 한국어는 토큰화가 어렵다.'\n",
        "\n",
        "우선 한국어는 띄어쓰기가 잘 안이루어 진다. 맞춤법도 잘 안이루어진다. 영어랑 다르게 독립적인 단어가 아닌 조사가 붙어 있는 경우가 많다. 그렇기 때문에 한국어 분석을 하려거든 형태소 개념을 파악하고 한국어의 자립형태소와 의존형태소를 고려해 토큰화 해야한다.\n",
        "\n",
        "쉽게말해 어절토큰화가 아니라 형태소 토큰화 해라.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mkwayLDroHt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK로 어간 추출해보기 (Stemming)\n",
        "\n",
        "다양한 어간 추출기가 존재하지만 앞에서 사용해본 NLTK에서 제공하는 스테머로 학습해보자."
      ],
      "metadata": {
        "id": "ubXXZlgTekQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import PorterStemmer # 단순 규칙에 의해 나눠진 것이므로 사전에 없는 단어가 찍힐 수있음.\n",
        "stemmer = PorterStemmer() #여러번 테스트 하기위해 스테머  객체 하나 생성해서 stemmer 라는 변수에 담아서 재사용하자.\n",
        "\n",
        "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'), stemmer.stem('happiest'), stemmer.stem('happying'))\n",
        "print(stemmer.stem('was'), stemmer.stem('loving'))\n"
      ],
      "metadata": {
        "id": "iXzZbofDulzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import LancasterStemmer # 또 다른 스테머 Porter랑 결과 다름 왜 다른지는 nltk 홈페이지를 참고하자\n",
        "stemmer = LancasterStemmer()\n",
        "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'), stemmer.stem('happiest'), stemmer.stem('happying'))\n",
        "print(stemmer.stem('was'), stemmer.stem('loving'))"
      ],
      "metadata": {
        "id": "XULMFXyjwNMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import RegexpStemmer # 사용자가 지정한 정규 표현 방식으로 어간 추출함\n",
        "stemmer = RegexpStemmer('ing') #ing 처리\n",
        "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'), stemmer.stem('happiest'), stemmer.stem('happying'))\n",
        "print(stemmer.stem('was'), stemmer.stem('loving'), stemmer.stem('sing')) #sing 에서 ing 를 빼니 s만 남음 ㅋ"
      ],
      "metadata": {
        "id": "BZzUg1kRxFJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import SnowballStemmer # 영어 외 13개 국어 stemming\n",
        "stemmer = SnowballStemmer('spanish') #스페인어\n",
        "print(stemmer.stem('hola')) #홀라의 어간은 hol 인가?"
      ],
      "metadata": {
        "id": "TEVolr7TxnZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import SnowballStemmer\n",
        "stemmer = SnowballStemmer('english')\n",
        "print(stemmer.stem('fishing'),stemmer.stem('fished'),stemmer.stem('fishs'))"
      ],
      "metadata": {
        "id": "uqSZEO9VyGCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "S3k9MtJx3FDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK로 표제어 추출해보기 (Lemmatization)\n",
        "\n",
        "표제어(Lemma)는 한글로 '기본 사전형 단어' 정도의 의미를 갖는다.\n",
        "\n",
        "am are is 는 다른 형태를 가졌지만 뿌리는 be 로 같습니다.\n",
        "\n",
        "* 형태소: 의미를 가진 가장 작은 단위!! 혼동하지 말것. (난 좀 자주 헷갈린다)\n"
      ],
      "metadata": {
        "id": "6zs8budQyojV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import WordNetLemmatizer\n",
        "# 표제어 추출에 필요한 데이터 다운로드\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "QA7cJpGiymoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemma = WordNetLemmatizer()\n",
        "# v는 단어가 동사일 때 동사의 기본형을 얻는다.\n",
        "# a는 단어가 형용사일 때 형용사의 기본형을 얻는다.\n",
        "print(lemma.lemmatize('amusing'),lemma.lemmatize('amuses'),lemma.lemmatize('amused'))\n",
        "print(lemma.lemmatize('happier'),lemma.lemmatize('happiest'))\n",
        "print(lemma.lemmatize('fancier'),lemma.lemmatize('fanciest'))\n",
        "print(lemma.lemmatize('was'), lemma.lemmatize('love'))\n",
        "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
        "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
        "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))\n",
        "print(lemma.lemmatize('was', 'v'), lemma.lemmatize('love', 'v'))\n",
        "print(lemma.lemmatize('dies','v'))\n",
        "print(lemma.lemmatize('watched', 'v'))\n",
        "print(lemma.lemmatize('has', 'v'))"
      ],
      "metadata": {
        "id": "xGBHyuP80dZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zFMwsu0uWL_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "정규표현식을 이용한 토큰화를 알아보자\n",
        "\n",
        "그 전에 정규 표현식 문법을 알아야한다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "정규 표현식 문법\n",
        "\n",
        "<figure>\n",
        "<img src = 'https://miro.medium.com/max/700/1*Y-q0dkUClSW0dX6uuysnJQ.png'>\n",
        "<figure>\n"
      ],
      "metadata": {
        "id": "5SPxK2ZM3JZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src = 'https://miro.medium.com/max/700/1*c__WeRlFyGY_-7LeKQkNnQ.png'>\n",
        "<figure>\n"
      ],
      "metadata": {
        "id": "5StEWSH64dEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<figure>\n",
        "<img src = 'https://miro.medium.com/max/661/1*6pIp6zuIoRHUOXjEDDUNCA.png'>\n",
        "<figure>\n"
      ],
      "metadata": {
        "id": "6fUiulNzOowY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#정규 표현식을 사용하기 위한 라이브러리 호출\n",
        "import re"
      ],
      "metadata": {
        "id": "zaUNoRLm6CiD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# . 은 한 개의 임의의 문자를 나타낸다.\n",
        "# 예를 들어 정규 표현식이 a.c라고 한다면, a와 c 사이에 어떤 1개의 문자라도 올 수 있다는 뜻이다.'\n",
        "r = re.compile(\"a.c\")\n",
        "print(r.search(\"kkk\")) #매치 안됨.\n",
        "print(r.search(\"abc\")) #매치 됨."
      ],
      "metadata": {
        "id": "Z6RC27yC5X90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ? 는 ? 앞의 문자가 존재할 수도 있고, 존재하지 않을 수도 있는 경우를 나타낸다.\n",
        "# 예를 들어 정규 표현식이 ab?C라고 한다면, b는 있다고 취급할 수도 있고 없다고 취급할 수도 있다. 즉, abc와 ac 모두 매치 가능\n",
        "r = re.compile(\"ab?c\")\n",
        "print(r.search(\"abbc\")) #매치 안됨.\n",
        "print(r.search(\"abc\")) #매치 됨.\n",
        "print(r.search(\"ac\")) #매치 됨."
      ],
      "metadata": {
        "id": "RaDFY4tm5qkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# * 는 바로 앞의 문자가 0개 이상일 경우를 나타낸다.\n",
        "# 앞의 문자는 존재하지 않을 수도 있으며, 또는 여러 개일 수도 있다.\n",
        "# 예를 들어 정규 표현식이 abc라고 한다면 ac, abc, abbc, abbbc 등과 매치할 수 있다.\n",
        "r = re.compile(\"ab*c\")\n",
        "print(r.search(\"a\"))\n",
        "print(r.search(\"ac\"))\n",
        "print(r.search(\"abc\"))\n",
        "print(r.search(\"abbc\"))"
      ],
      "metadata": {
        "id": "czUUK5eu5xYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# + 는 *와 유사하다.\n",
        "# 다른 점은 앞의 문자가 모두 있어야 한다.\n",
        "# 예를 들어 ab+c라고 하면, ac는 매치되지 않는다\n",
        "r = re.compile('ab+c')\n",
        "print(r.search(\"ac\"))# 아무것도 출력되지 않는다.\n",
        "print(r.search(\"abc\"))"
      ],
      "metadata": {
        "id": "qvd7jyMD89xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ^ 는 시작되는 글자를 지정한다.\n",
        "# 예를 들어 ^a라면 a로 시작되는 문자열만 찾는다\n",
        "r = re.compile('^a')\n",
        "print(r.search('bbc')) # 아무것도 출력되지 않음\n",
        "print(r.search('ab'))"
      ],
      "metadata": {
        "id": "oDjUr-PX9wF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# $는 마지막 글자를 지정한다.\n",
        "# 예를 들어 $a라면 a로 끝나는 문자열을 찾는다\n",
        "r = re.compile('a$')\n",
        "print(r.search('aasdasdasd')) #아무것도 출력되지 않음\n",
        "print(r.search('aasdasdasda'))"
      ],
      "metadata": {
        "id": "d9fnTC9s-ENv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자{숫자1, 숫자2}\n",
        "# 문자에 해당 기호를 붙이면, 해당 문자를 숫자1 이상 숫자2 이하만큼 반복한다.\n",
        "# 예를 들어 정규 표현식이 ab{2,8}c라면 a와 c사이에 b가 존재하면서 b는 2개 이상 8개 이하인 문자열에 대해서 매치한다.\n",
        "r = re.compile(\"ab{2,8}c\")\n",
        "print(r.search('ac'))# 아무런 결과도 출력되지 않음\n",
        "print(r.search(\"abc\"))# 아무런 결과도 출력되지 않음\n",
        "print(r.search(\"abbc\"))\n",
        "print(r.search(\"abbbbc\"))\n",
        "print(r.search(\"abbbbbbbbbbbc\")) # 아무런 결과도 출력되지 않음"
      ],
      "metadata": {
        "id": "krjEzbi7-lM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자{숫자1,}\n",
        "# 문자에 해당 기호를 붙이면, 해당 문자를 숫자 이상만큼 반복한다.\n",
        "# 예를 들어 정규 표현식이 a{2,}bc라면 뒤에 bc가 붙으면서 a의 갯수가 2개 이상인 경우인 문자열과 매치한다.\n",
        "# 또한 만약 {0,}을 쓴다면 *와 동일한 의미가 되며, {1,}을 쓴다면 +와 동일한 의미가 된다.\n",
        "r = re.compile(\"a{2,}bc\")\n",
        "print(r.search(\"bc\")) # 아무런 결과도 출력되지 않음\n",
        "print(r.search(\"aa\")) # 아무런 결과도 출력되지 않음\n",
        "print(r.search(\"aabc\"))\n",
        "print(r.search(\"aaaaabc\"))"
      ],
      "metadata": {
        "id": "ZW-xz5sG-nyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [ ]\n",
        "# []안에 문자들을 넣으면 그 문자들 중 한 개의 문자와 매치라는 의미를 가진다.\n",
        "# 예를 들어 정규 표현식이 [abc]라면, a 또는 b 또는 c가 들어가 있는 문자열과 매치된다.\n",
        "# 범위를 지정하는 것도 가능하다. [a-zA-Z]는 알파벳 전부를 의미하며, [0–9]는 숫자 전부를 의미한다.\n",
        "r = re.compile(\"[abc]\")\n",
        "print(r.search('zzz'))# 아무것도 출력되지 않음\n",
        "print(r.search('a'))"
      ],
      "metadata": {
        "id": "ST9YBDEi-pfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[a-c] : [abc]와 같음\n",
        "\n",
        "[0-5] : [012345]와 같음\n",
        "\n",
        "[a-zA-Z] : 모든 알파벳\n",
        "\n",
        "[0-9] : 모든 숫자"
      ],
      "metadata": {
        "id": "bzPeAKpqAC4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ^[문]\n",
        "# 앞서 설명한 ^와는 완전히 다른 의미로 쓰인다.\n",
        "# 여기서는 ^ 기호 뒤에 붙은 문자들을 제외한 모든 문자를 매치하는 역할을 한다.\n",
        "# 예를 들어 [^abc]라는 정규 표현식이 있다면, a 또는 b 또는 c가 들어간 문자열을 제외한 모든 문자열을 매치한다.\n",
        "\n",
        "r = re.compile('[^abc]')\n",
        "print(r.search(\"a\")) # 아무것도 출력되지 않음\n",
        "print(r.search(\"ahoho\"))"
      ],
      "metadata": {
        "id": "Pukn8zUo-siB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "정규 표현식 모듈 써보기\n",
        "\n"
      ],
      "metadata": {
        "id": "MMQqpGEUzVCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# re.match() 와 re.search() 차이 알아보기\n",
        "# search는 정규표현식 전체에 대해서 문자열이 매치하는지 본다.\n",
        "# match는 문자열 첫 부분부터 정규표현식과 매치하는지 본다.\n",
        "\n",
        "r = re.compile(\"ab.\")\n",
        "print(r.search(\"kkkabc\"))\n",
        "print(r.match(\"kkkabc\")) # 출력 x\n",
        "print(r.match(\"abckkk\"))"
      ],
      "metadata": {
        "id": "Wf2cFz4SzUja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# re.split() 은 입력된 정규 표현식을 기준으로 문자열들을 분리하여 리스트로 리턴한다.\n",
        "# 토큰화에 유용하게 쓰일 수 있기 때문에 자연어 처리에서 가장 많이 사용되는 정규 표현식 함수 중 하나.\n",
        "text = \"사과 딸기 수박 멜론 바나나\"\n",
        "print(re.split(\" \",text))\n",
        "\n",
        "text = \"\"\"사과\n",
        "딸기\n",
        "수박\n",
        "멜론\n",
        "바나나\"\"\"\n",
        "print(re.split(\"\\n\", text))\n",
        "\n",
        "text = \"사과+딸기+수박+멜론+바나나\"\n",
        "print(re.split(\"\\+\", text))\n",
        "print(text.split('+'))"
      ],
      "metadata": {
        "id": "KFSN3B_-0vVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# re.findall() 은 정규 표현식과 매치되는 모든 문자열들을 리스트로 리턴한다.\n",
        "# 단, 매치되는 문자열이 없다면 빈 리스트를 리턴한다.\n",
        "\n",
        "text = \"\"\"이름 : 김철수\n",
        "전화번호 : 010 - 1234 - 1234\n",
        "나이 : 30\n",
        "성별 : 남\"\"\"\n",
        "print(re.findall(\"\\d+\", text))\n",
        "print(re.findall(\"\\d+\", \"문자열입니당.\")) # 빈 리스트를 리턴했다."
      ],
      "metadata": {
        "id": "KcFv1qo414f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9lLrKKOS2O_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "65ozVYY4LPW_"
      }
    }
  ]
}